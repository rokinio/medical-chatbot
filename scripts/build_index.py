#!/usr/bin/env python3
# scripts/build_index.py
"""
Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹ Ù…ØªÙ†
- Ù¾Ø´ØªÛŒØ¨Ø§Ù†ÛŒ Ø§Ø² Ø­Ø¬Ù… Ø¨Ø§Ù„Ø§ÛŒ Ø¯Ø§Ø¯Ù‡ (ØªØ§ 50,000+ Ø³ÙˆØ§Ù„)
- Ø¨Ù‡ÛŒÙ†Ù‡â€ŒØ³Ø§Ø²ÛŒ Ø¨Ø§ Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² Ø¨Ú†â€ŒÙ‡Ø§ Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡
- Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² IndexIVFFlat Ø¨Ø±Ø§ÛŒ Ú©Ø§Ø±Ø§ÛŒÛŒ Ø¨Ù‡ØªØ± Ø¨Ø§ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ Ø³ÙˆØ§Ù„
"""

import json
import os
import argparse
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
from datetime import datetime
from tqdm import tqdm

def load_questions(input_file):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø§Ø² ÙØ§ÛŒÙ„ JSON"""
    try:
        print(f"ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³ÙˆØ§Ù„Ø§Øª Ø§Ø² {input_file}...")
        with open(input_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        questions = [entry["question"] for entry in data]
        print(f"âœ… {len(questions)} Ø³ÙˆØ§Ù„ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
        return questions
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ø³ÙˆØ§Ù„Ø§Øª: {e}")
        return []

def build_index(questions, model_name, output_dir, batch_size=32, use_ivf=True):
    """Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø¨Ø±Ø§ÛŒ Ø³ÙˆØ§Ù„Ø§Øª"""
    if not questions:
        print("âŒ Ù‡ÛŒÚ† Ø³ÙˆØ§Ù„ÛŒ Ø¨Ø±Ø§ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³â€ŒØ³Ø§Ø²ÛŒ ÛŒØ§ÙØª Ù†Ø´Ø¯.")
        return False
    
    try:
        # ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ± Ø®Ø±ÙˆØ¬ÛŒ
        os.makedirs(output_dir, exist_ok=True)
        index_path = os.path.join(output_dir, "faq_index.faiss")
        metadata_path = os.path.join(output_dir, "faq_metadata.json")
        
        # ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ± Ú©Ø´ Ù…Ø¯Ù„
        model_cache_dir = os.path.join(os.path.dirname(output_dir), "models")
        os.makedirs(model_cache_dir, exist_ok=True)
        
        # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„
        print(f"ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù…Ø¯Ù„ {model_name}...")
        model = SentenceTransformer(model_name, cache_folder=model_cache_dir)
        
        # Ø§ÛŒØ¬Ø§Ø¯ embeddings Ø¨Ù‡ ØµÙˆØ±Øª Ø¨Ú† Ø¨Ø±Ø§ÛŒ Ú©Ø§Ù‡Ø´ Ù…ØµØ±Ù Ø­Ø§ÙØ¸Ù‡
        print(f"ğŸ”„ Ø³Ø§Ø®Øª embeddings Ø¨Ø§ Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ú† {batch_size}...")
        embeddings = []
        
        for i in tqdm(range(0, len(questions), batch_size)):
            batch = questions[i:i+batch_size]
            batch_embeddings = model.encode(batch)
            embeddings.append(batch_embeddings)
        
        # ØªØ±Ú©ÛŒØ¨ Ù‡Ù…Ù‡ embeddings
        all_embeddings = np.vstack(embeddings)
        dimension = all_embeddings.shape[1]
        print(f"âœ… embeddings Ø¨Ø§ Ø§Ø¨Ø¹Ø§Ø¯ {all_embeddings.shape} Ø³Ø§Ø®ØªÙ‡ Ø´Ø¯.")
        
        # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù…Ù†Ø§Ø³Ø¨ Ø¨Ø± Ø§Ø³Ø§Ø³ ØªØ¹Ø¯Ø§Ø¯ Ø³ÙˆØ§Ù„Ø§Øª
        if use_ivf and len(questions) > 1000:
            # Ù…Ø­Ø§Ø³Ø¨Ù‡ ØªØ¹Ø¯Ø§Ø¯ Ú©Ù„Ø§Ø³ØªØ±Ù‡Ø§
            nlist = min(int(len(questions) / 39), 1024)  # Ø­Ø¯Ø§Ú©Ø«Ø± 1024 Ú©Ù„Ø§Ø³ØªØ±
            nlist = max(nlist, 4)  # Ø­Ø¯Ø§Ù‚Ù„ 4 Ú©Ù„Ø§Ø³ØªØ±
            
            print(f"ğŸ”„ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ IndexIVFFlat Ø¨Ø§ {nlist} Ú©Ù„Ø§Ø³ØªØ±...")
            quantizer = faiss.IndexFlatL2(dimension)
            index = faiss.IndexIVFFlat(quantizer, dimension, nlist)
            
            # Ø¢Ù…ÙˆØ²Ø´ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ù‚Ø¨Ù„ Ø§Ø² Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
            if not index.is_trained:
                index.train(all_embeddings)
            
            # ØªÙ†Ø¸ÛŒÙ… Ù¾Ø§Ø±Ø§Ù…ØªØ± nprobe Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø¨Ù‡ØªØ±
            nprobe = min(nlist // 4, 16)  # Ø­Ø¯Ø§Ú©Ø«Ø± 16
            nprobe = max(nprobe, 1)  # Ø­Ø¯Ø§Ù‚Ù„ 1
            index.nprobe = nprobe
            
            print(f"âœ… Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø§ nprobe={nprobe} ØªÙ†Ø¸ÛŒÙ… Ø´Ø¯.")
        else:
            print(f"ğŸ”„ Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ IndexFlatL2...")
            index = faiss.IndexFlatL2(dimension)
        
        # Ø§Ø¶Ø§ÙÙ‡ Ú©Ø±Ø¯Ù† Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        index.add(all_embeddings)
        print(f"âœ… {len(questions)} Ø³ÙˆØ§Ù„ Ø¨Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø§Ø¶Ø§ÙÙ‡ Ø´Ø¯.")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³
        print(f"ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¯Ø± {index_path}...")
        faiss.write_index(index, index_path)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§
        print(f"ğŸ’¾ Ø°Ø®ÛŒØ±Ù‡ Ù…ØªØ§Ø¯ÛŒØªØ§ Ø¯Ø± {metadata_path}...")
        metadata = {
            "questions": questions,
            "model_name": model_name,
            "created_at": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "dimension": dimension,
            "count": len(questions)
        }
        
        if use_ivf and len(questions) > 1000:
            metadata["index_type"] = "IndexIVFFlat"
            metadata["nlist"] = nlist
            metadata["nprobe"] = index.nprobe
        else:
            metadata["index_type"] = "IndexFlatL2"
        
        with open(metadata_path, "w", encoding="utf-8") as f:
            json.dump(metadata, f, ensure_ascii=False, indent=4)
        
        print("âœ… Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        return True
    
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ FAISS Ø¨Ø±Ø§ÛŒ Ø¬Ø³ØªØ¬ÙˆÛŒ Ø³Ø±ÛŒØ¹ Ù…ØªÙ†")
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ
    parser.add_argument("--input", type=str, default="../app/data/processed_all_questions.json", help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø³ÙˆØ§Ù„Ø§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø´Ø¯Ù‡")
    parser.add_argument("--output-dir", type=str, default="../app/data", help="Ù…Ø³ÛŒØ± Ù¾ÙˆØ´Ù‡ Ø®Ø±ÙˆØ¬ÛŒ")
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ù„
    parser.add_argument("--model", type=str, default="sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2", help="Ù†Ø§Ù… Ù…Ø¯Ù„ Sentence Transformer")
    parser.add_argument("--batch-size", type=int, default=32, help="Ø§Ù†Ø¯Ø§Ø²Ù‡ Ø¨Ú† Ø¨Ø±Ø§ÛŒ Ø³Ø§Ø®Øª embeddings")
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ Ø§ÛŒÙ†Ø¯Ú©Ø³
    parser.add_argument("--no-ivf", action="store_true", help="Ø¹Ø¯Ù… Ø§Ø³ØªÙØ§Ø¯Ù‡ Ø§Ø² IndexIVFFlat Ø­ØªÛŒ Ø¨Ø±Ø§ÛŒ ØªØ¹Ø¯Ø§Ø¯ Ø²ÛŒØ§Ø¯ Ø³ÙˆØ§Ù„")
    
    args = parser.parse_args()
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø³ÙˆØ§Ù„Ø§Øª
    questions = load_questions(args.input)
    
    # Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³
    if questions:
        use_ivf = not args.no_ivf
        success = build_index(questions, args.model, args.output_dir, args.batch_size, use_ivf)
        
        if success:
            print("âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        else:
            print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¹Ù…Ù„ÛŒØ§Øª Ø³Ø§Ø®Øª Ø§ÛŒÙ†Ø¯Ú©Ø³.")
    else:
        print("âŒ Ø¨Ù‡ Ø¯Ù„ÛŒÙ„ Ø¹Ø¯Ù… ÙˆØ¬ÙˆØ¯ Ø³ÙˆØ§Ù„ØŒ Ø¹Ù…Ù„ÛŒØ§Øª Ù…ØªÙˆÙ‚Ù Ø´Ø¯.")

if __name__ == "__main__":
    main()