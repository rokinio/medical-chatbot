#!/usr/bin/env python3
# scripts/process_data.py
"""
Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªÙ…ÛŒØ²Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨
- Ø­Ø°Ù Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø®ØµÛŒ (Ù†Ø§Ù…â€ŒÙ‡Ø§ØŒ Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ†â€ŒÙ‡Ø§ØŒ Ø§ÛŒÙ…ÛŒÙ„â€ŒÙ‡Ø§)
- Ø­Ø°Ù Ù¾ÛŒØ§Ù…â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
- Ø§Ø³ØªØ§Ù†Ø¯Ø§Ø±Ø¯Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
"""

import json
import re
import os
import argparse
import pandas as pd
from collections import Counter
from datetime import datetime
from tqdm import tqdm

def load_names(names_file):
    """Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù„ÛŒØ³Øª Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø² ÙØ§ÛŒÙ„ CSV"""
    try:
        print(f"ğŸ“š Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ Ø§Ø² {names_file}...")
        df = pd.read_csv(names_file)
        names = set(df.iloc[:, 0].dropna().str.strip())  # ÙØ±Ø¶ Ø¨Ø± Ø§ÛŒÙ† Ø§Ø³Øª Ú©Ù‡ Ù†Ø§Ù…â€ŒÙ‡Ø§ Ø¯Ø± Ø³ØªÙˆÙ† Ø§ÙˆÙ„ Ù‡Ø³ØªÙ†Ø¯
        print(f"âœ… {len(names)} Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ø´Ø¯.")
        return names
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ ÙØ§ÛŒÙ„ Ù†Ø§Ù…â€ŒÙ‡Ø§: {e}")
        return set()

def get_excluded_names():
    """Ù„ÛŒØ³Øª Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒÛŒ Ú©Ù‡ Ø¨Ø§ÛŒØ¯ Ù†Ø§Ø¯ÛŒØ¯Ù‡ Ú¯Ø±ÙØªÙ‡ Ø´ÙˆÙ†Ø¯"""
    return {
        "Ù…Ù†ÙŠ", "Ø¯Ø¹Ø§", "Ù†ÙˆØ±", "Ø¹ÙˆØ¶", "ØµÙØ±", "Ø¹Ø²Øª", "Ù…Ø§Ù‡", "Ø¨Ú¯Ù…", "Ø³Ø¨Ø§", "Ø´ÙƒØ±", "ÙƒØ±Ù…", 
        "Ø¢Ø°Ø±", "Ù‡Ù…Øª", "ØµÙØ§", "Ø´ÙÙ‚", "Ù‡ÙˆØ§", "Ø¹Ø³Ù„", "Ø¨Ù‡Ø§Ø±", "Ù…Ø­Ø±Ù…", "Ø¬Ù…Ø¹Ù‡", "ØµÙˆØ±Øª", 
        "Ù‡Ù„Ø§Ù„", "Ø¯ÙØªØ±", "Ø­Ø§ÙØ¸", "Ø±ÙˆØ²ÙŠ", "ØªØ±Ø§Ø¨", "Ø§ØµÙ„ÙŠ", "ÙØ±ØµØª", "Ù…Ø­Ø¨Øª", "Ù‚Ø³Ù…Øª", 
        "Ù…Ø§Ù‡Ù‡", "Ù„Ø§Ø²Ù…", "ØªØ±Ø­Ù…", "ÙƒØ´ÙˆØ±", "Ø¨Ø§Ù†Ùˆ", "Ù†Ø¸Ø§Ù…", "Ø¨Ù‡Ù…Ù†", "Ø¬Ù‡Ø§Ù†", "Ø®Ø§Ù†Ù…", 
        "Ø¯Ø§Ù†Ø´", "Ù¾Ø±ØªÙˆ", "ÙŠØ³Ø±ÙŠ", "Ù…Ù„ÙƒÙŠ", "ÙƒØ§Ù…Ù„", "Ù…Ø·Ù„Ø¨", "Ø¯Ø®ØªØ±", "Ø²Ù…Ø§Ù†", "Ø±Ø§Ø¶ÙŠ", 
        "Ø¨Ø§Ù„Ø§", "Ø´Ø§Ù‡Ø¯", "ØµØ§Ø­Ø¨", "Ø³Ø§Ù„Ù…", "Ú¯Ù„Ø§Ø¨", "Ø´Ù†Ø¨Ù‡", "ÙƒÙˆÚ†Ùƒ", "Ø¨Ø²Ø±Ú¯", "Ù†Ø¨Ø§Øª", 
        "Ø¢Ø²Ø§Ø¯", "Ø±Ù…Ø¶Ø§Ù†", "ØªÙ…Ø§Ø´Ø§", "Ú¯ÙŠÙ„Ø§Ù†", "Ø¨Ù‡Ø¨ÙˆØ¯", "ÙØ±Ù…Ø§Ù†", "Ù…Ù†ØªÙ‡ÙŠ", "Ø§Ø±Ø´Ø§Ø¯", 
        "Ù†ÙˆØ±ÙˆØ²", "Ø³Ø¹Ø§Ø¯Øª", "Ø§ÙŠØ±Ø§Ù†", "Ù…Ø¹Ø±ÙˆÙ", "Ø´Ø±Ø§ÙØª", "ØªØ§Ø¨Ø§Ù†", "Ø³Ù„Ø§Ù…Øª", "Ø§Ù„Ø¨Ø±Ø²", 
        "Ø­Ø§ÙØ¸Ù‡", "Ù…Ù†ØªØ¸Ø±", "Ø§Ø­ØªØ±Ø§Ù…", "Ø§Ù†ØªØ¸Ø§Ø±", "Ù…Ø§Ù†Ø¯Ú¯Ø§Ø±"
    }

def clean_text(text, iranian_names, excluded_names, phone_counter, removed_names):
    """Ø­Ø°Ù Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø­Ø³Ø§Ø³ Ùˆ Ù†Ø§Ù…â€ŒÙ‡Ø§ Ø§Ø² Ù…ØªÙ†"""
    if not text or text.strip() == "":
        return ""

    original_text = text  # Ø°Ø®ÛŒØ±Ù‡ Ù…ØªÙ† Ø§ØµÙ„ÛŒ Ø¨Ø±Ø§ÛŒ Ø¨Ø±Ø±Ø³ÛŒ Ù„Ø§Ú¯

    # Ø­Ø°Ù Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ† (ÙØ±Ù…Øª 09xx xxx xxxx ÛŒØ§ 09xx-xxx-xxxx)
    phone_matches = re.findall(r'09\d{2}[- ]?\d{3}[- ]?\d{4}', text)
    for phone in phone_matches:
        phone_counter[phone] += 1
    text = re.sub(r'09\d{2}[- ]?\d{3}[- ]?\d{4}', '[REDACTED]', text)

    # Ø­Ø°Ù Ø¢Ø¯Ø±Ø³ Ø§ÛŒÙ…ÛŒÙ„
    text = re.sub(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}', '[REDACTED]', text)

    # Ø­Ø°Ù Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø§ÛŒØ±Ø§Ù†ÛŒ Ø§Ø² Ù…ØªÙ† Ù…Ú¯Ø± Ø§ÛŒÙ†Ú©Ù‡ Ø¯Ø± Ù„ÛŒØ³Øª excluded_names Ø¨Ø§Ø´Ù†Ø¯
    words = re.findall(r'\w+', original_text)
    for word in words:
        cleaned_word = word.lower().strip()
        if cleaned_word in iranian_names and cleaned_word not in excluded_names:
            removed_names.add(cleaned_word)
            text = re.sub(rf'\b{re.escape(cleaned_word)}\b', '[REDACTED]', text)
    
    return text

def process_data(input_file, output_file, names_file, log_dir=None):
    """Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ Ùˆ ØªÙ…ÛŒØ²Ø³Ø§Ø²ÛŒ Ø¢Ù†"""
    # ØªÙ†Ø¸ÛŒÙ… Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯
    if not log_dir:
        log_dir = os.path.join(os.path.dirname(output_file), "logs")
    os.makedirs(log_dir, exist_ok=True)
    
    # Ø¨Ø§Ø±Ú¯Ø°Ø§Ø±ÛŒ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ
    iranian_names = load_names(names_file)
    excluded_names = get_excluded_names()
    
    # Ø¢Ù…Ø§Ø¯Ù‡â€ŒØ³Ø§Ø²ÛŒ Ù…ØªØºÛŒØ±Ù‡Ø§ÛŒ Ø¢Ù…Ø§Ø±Ú¯ÛŒØ±ÛŒ
    phone_counter = Counter()
    removed_names = set()
    removed_duplicates = 0
    
    try:
        # Ø®ÙˆØ§Ù†Ø¯Ù† Ø¯Ø§Ø¯Ù‡ Ø®Ø§Ù…
        print(f"ğŸ“‚ Ø®ÙˆØ§Ù†Ø¯Ù† ÙØ§ÛŒÙ„ {input_file}...")
        with open(input_file, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        print(f"ğŸ” Ù¾Ø±Ø¯Ø§Ø²Ø´ {len(data)} Ø³ÙˆØ§Ù„ Ùˆ Ù¾Ø§Ø³Ø®...")
        
        unique_messages = set()  # Ø°Ø®ÛŒØ±Ù‡ Ø³ÙˆØ§Ù„Ø§Øª Ùˆ Ù¾Ø§Ø³Ø®â€ŒÙ‡Ø§ÛŒ ØªÚ©Ø±Ø§Ø±ÛŒ
        
        # Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
        for entry in tqdm(data):
            new_conversation = []
            for message in entry["conversation"]:
                cleaned_content = clean_text(
                    message["message"], 
                    iranian_names, 
                    excluded_names, 
                    phone_counter, 
                    removed_names
                )
                
                if cleaned_content and cleaned_content not in unique_messages:
                    unique_messages.add(cleaned_content)
                    new_conversation.append({"sender": message["sender"], "message": cleaned_content})
                else:
                    removed_duplicates += 1
            
            entry["conversation"] = new_conversation
        
        # Ø­Ø°Ù Ø³ÙˆØ§Ù„Ø§ØªÛŒ Ú©Ù‡ Ù…Ú©Ø§Ù„Ù…Ù‡ Ø®Ø§Ù„ÛŒ Ø¯Ø§Ø±Ù†Ø¯
        original_count = len(data)
        data = [entry for entry in data if entry["conversation"]]
        empty_conversations = original_count - len(data)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ø®Ø±ÙˆØ¬ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´â€ŒØ´Ø¯Ù‡
        os.makedirs(os.path.dirname(output_file), exist_ok=True)
        with open(output_file, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=4)
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ùâ€ŒØ´Ø¯Ù‡
        names_log_file = os.path.join(log_dir, "removed_names.txt")
        with open(names_log_file, "w", encoding="utf-8") as f:
            f.write("Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ Ø­Ø°Ù Ø´Ø¯Ù‡ (Ù…Ø±ØªØ¨ Ø´Ø¯Ù‡ Ø¨Ø± Ø§Ø³Ø§Ø³ Ø·ÙˆÙ„):\n")
            sorted_removed_names = sorted(removed_names, key=len)
            for name in sorted_removed_names:
                f.write(f"{name} ({len(name)} Ú©Ø§Ø±Ø§Ú©ØªØ±)\n")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯ Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ„ÙÙ† Ø­Ø°Ùâ€ŒØ´Ø¯Ù‡
        phone_log_file = os.path.join(log_dir, "phone_numbers.txt")
        with open(phone_log_file, "w", encoding="utf-8") as f:
            f.write("Ø´Ù…Ø§Ø±Ù‡â€ŒÙ‡Ø§ÛŒ ØªÙ„ÙÙ† Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ø´Ø¯Ù‡:\n")
            for phone, count in phone_counter.most_common():
                f.write(f"{phone}: {count} Ù…ÙˆØ±Ø¯\n")
        
        # Ø°Ø®ÛŒØ±Ù‡ Ù„Ø§Ú¯ Ø¢Ù…Ø§Ø±ÛŒ
        stats_file = os.path.join(log_dir, "processing_stats.json")
        stats = {
            "processing_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
            "original_records": original_count,
            "processed_records": len(data),
            "empty_conversations_removed": empty_conversations,
            "duplicate_messages_removed": removed_duplicates,
            "unique_names_removed": len(removed_names),
            "phone_numbers_found": len(phone_counter)
        }
        
        with open(stats_file, "w", encoding="utf-8") as f:
            json.dump(stats, f, ensure_ascii=False, indent=4)
        
        print(f"âœ… Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
        print(f"âœ… {len(data)} Ø³ÙˆØ§Ù„ Ùˆ Ù¾Ø§Ø³Ø® Ø¯Ø± ÙØ§ÛŒÙ„ {output_file} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯.")
        print(f"âœ… {removed_duplicates} Ù¾ÛŒØ§Ù… ØªÚ©Ø±Ø§Ø±ÛŒ Ø­Ø°Ù Ø´Ø¯.")
        print(f"âœ… {len(removed_names)} Ù†Ø§Ù… ÙØ§Ø±Ø³ÛŒ Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø­Ø°Ù Ø´Ø¯.")
        print(f"âœ… {len(phone_counter)} Ø´Ù…Ø§Ø±Ù‡ ØªÙ„ÙÙ† Ø´Ù†Ø§Ø³Ø§ÛŒÛŒ Ùˆ Ø­Ø°Ù Ø´Ø¯.")
        print(f"âœ… ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯ Ø¯Ø± Ù¾ÙˆØ´Ù‡ {log_dir} Ø°Ø®ÛŒØ±Ù‡ Ø´Ø¯Ù†Ø¯.")
        
        return True
    
    except Exception as e:
        print(f"âŒ Ø®Ø·Ø§ Ø¯Ø± Ù¾Ø±Ø¯Ø§Ø²Ø´ ÙØ§ÛŒÙ„: {e}")
        return False

def main():
    parser = argparse.ArgumentParser(description="Ù¾Ø±Ø¯Ø§Ø²Ø´ Ùˆ ØªÙ…ÛŒØ²Ø³Ø§Ø²ÛŒ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§ÛŒ Ø³ÙˆØ§Ù„ Ùˆ Ø¬ÙˆØ§Ø¨")
    
    # Ù¾Ø§Ø±Ø§Ù…ØªØ±Ù‡Ø§ÛŒ ÙˆØ±ÙˆØ¯ÛŒ Ùˆ Ø®Ø±ÙˆØ¬ÛŒ
    parser.add_argument("--input", type=str, default="../app/data/all_questions.json", help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ ÙˆØ±ÙˆØ¯ÛŒ")
    parser.add_argument("--output", type=str, default="../app/data/processed_all_questions.json", help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ø®Ø±ÙˆØ¬ÛŒ")
    parser.add_argument("--names-file", type=str, default="name.csv", help="Ù…Ø³ÛŒØ± ÙØ§ÛŒÙ„ Ù†Ø§Ù…â€ŒÙ‡Ø§ÛŒ ÙØ§Ø±Ø³ÛŒ")
    parser.add_argument("--log-dir", type=str, help="Ù…Ø³ÛŒØ± Ø°Ø®ÛŒØ±Ù‡ ÙØ§ÛŒÙ„â€ŒÙ‡Ø§ÛŒ Ù„Ø§Ú¯")
    
    args = parser.parse_args()
    
    # Ø§Ø¬Ø±Ø§ÛŒ Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¯Ø§Ø¯Ù‡â€ŒÙ‡Ø§
    success = process_data(args.input, args.output, args.names_file, args.log_dir)
    
    if success:
        print("âœ… Ø¹Ù…Ù„ÛŒØ§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´ Ø¨Ø§ Ù…ÙˆÙÙ‚ÛŒØª Ø§Ù†Ø¬Ø§Ù… Ø´Ø¯.")
    else:
        print("âŒ Ø®Ø·Ø§ Ø¯Ø± Ø¹Ù…Ù„ÛŒØ§Øª Ù¾Ø±Ø¯Ø§Ø²Ø´.")

if __name__ == "__main__":
    main()